---
breadcrumb: RabbitMQ for Pivotal Cloud Foundry Documentation
title: Deploying the RabbitMQ&reg; Service
owner: London Services
---

## <a id="default"></a>Default Deployment

Deploying RabbitMQ for [Pivotal Cloud Foundry](https://network.pivotal.io/products/pivotal-cf) (PCF) through Ops Manager will deploy a RabbitMQ cluster of **3 nodes** by default.

The deployment includes a single load balancer `haproxy` which spreads connections on all of the default ports, for all of the shipped plugins across all of the machines within the cluster.

The deployment will occur in a single availability zone (AZ).

The default configuration is for testing purposes only and it is recommended that customers have a minium of **3 RabbitMQ nodes** and **2 HAProxy nodes**

<%= image_tag("images/deployment_default.jpeg") %>

### <a id="default-consider"></a>Considerations for this deployment

* Provides HA for the RabbitMQ cluster
* Queues must be judiciously configured to be HA as they are placed on one node by default
* Customers should decide on which partition behaviour is best suited to their use case.  For two nodes 'automatic' is preferred
* HAProxy is a single point of failure (SPOF)
* The entire deployment is in a single AZ, which does not protect against external failures from failures in hardware, networking, etc.

## <a id="recommended"></a>Recommended Deployment

We recommend that RabbitMQ is deployed across at least two availability zones.

RabbitMQ server nodes should be scaled to an odd number and should be greater than 3.

Replication of queues should only be used where required as it can have a big impact on system performance.

The HAProxy job instance count should also be increased to match the number of AZs to ensure there is a HAProxy located in each AZ.
This removes the HAProxy SPOF and provides further redundancy.

<%= image_tag("images/deployment_recommended.jpeg") %>

In the above diagram, you can see that you can now suffer the failure of a single HAProxy and single RabbitMQ node and still keep your cluster online and applications connected.

It is also recommend that customers chooses an odd number of RabbitMQ server nodes of three or more.

### <a id="upgrade-from-single-az"></a>Upgrading to this deployment from a single AZ deployment

It is **not** possible to upgrade to this setup from the default deployment across a single AZ.

This is because the AZ setup cannot be changed once the tile has being deployed for the first time, this is to protect against data loss when moving jobs between AZs.

### <a id="upgrade-from-multi-az"></a>Upgrading to this deployment from a multi AZ deployment

If you have deployed the tile across two AZs, but with a single HAProxy instance you can migrate to this setup as follows:

1. Deploy an additional HAProxy instance through OpsManager
2. New or re-bound applications to the RabbitMQ service will see the IPs of both HAProxys immediately
3. Existing bound applications will continue to work, but only using the previously deployed HAProxy IP Address. They can be re-bound as required at your discretion.

### <a id="recommended-consider"></a>Considerations for this deployment

* Requires IaaS configuration for availability zones ahead of deploying the RabbitMQ tile
* Application developers will be handed the IPs of each deployed HAProxy in their environment variables
* Queues must be judiciously configured to be HA as they are placed on one node by default
* Customers should decide on which partition behaviour is best suited to their use case.  For 3 or more nodes 'pause_minority' is preferred

## <a id="advanced"></a>Advanced Deployment

This deployment builds upon the above recommended deployment, so follows the same upgrade paths.

This allows you to replace the use of HAProxy with your own external load balancer.

You may choose to do this to remove any knowledge of the topology of the RabbitMQ setup from application developers.

<%= image_tag("images/deployment_advanced.jpeg") %>

Advantages

* Application developers do not need to handle multiple IPs for the HAProxy jobs in their applications

Disadvantages

* The load balancer needs to be configured with the IPs of the RabbitMQ Nodes. These will only be known once the deployment has finished. The IPs should remain the same during subsequent deployments but there is a risk they can change.


### <a id="upgrade-from-recommended"></a>Upgrading to this deployment from the recommended deployment

It is possible to first deploy with multiple HAProxy jobs, as per the recommended deployment and decided to later use your own external load balancer.

This can be achieved without downtime to your applications.

This can be achieved as follows:

1. Configure your external load balancer to point to the RabbitMQ Node IPs
2. Configure the DNS name or IP address for the external load balancer (ELB) on the RabbitMQ tile in OpsManager
3. Deploy the changes
4. Any new instances of the RabbitMQ service **or** any re-bound connections will use the DNS name or IP address of the ELB in their `VCAP_SERVICES`
5. Any existing instances will continue to use the HAProxy IP addresses in their `VCAP_SERVICES`
6. Phase the re-binding of existing applications to have their environment variables updated
7. Once all applications are updated
8. Reduce the instance count of the `HAProxy` job in OpsManager to 1
9. Deploy the changes

This approach works as any existing bound applications have their `VCAP_SERVICES` information cached in the cloud controller and are only updated by a re-bind request.

### <a id="downgrade-to-recommended"></a>Downgrading from this deployment to the recommended deployment

If you are currently using an external load balancer, then you can move back to using HAProxys instead.

You can achieve this by following the above steps in reverse order and re-instating the HAProxy jobs.

## <a id="resource-requirements"></a>Resource requirements

The following table shows the default resource and IP requirements for installing the tile:
<table border="1" class="nice">
	<tr>
		<th>Product</th>
		<th>Resource</th>
		<th>Instances</th>
		<th>CPU</th>
		<th>Ram</th>
		<th>Ephemeral</th>
		<th>Persistent</th>
		<th>Static IP</th>
		<th>Dynamic IP</th>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>RabbitMQ node</td>
		<td>3</td>
		<td>2</td>
		<td>8192</td>
		<td>16384</td>
		<td>30720</td>
		<td>1</td>
		<td>0</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>HAProxy for RabbitMQ</td>
		<td>1</td>
		<td>1</td>
		<td>2048</td>
		<td>4096</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>RabbitMQ service broker</td>
		<td>1</td>
		<td>1</td>
		<td>2048</td>
		<td>4096</td>
		<td>0</td>
		<td>1</td>
		<td>0</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>Broker Registrar</td>
		<td>1</td>
		<td>1</td>
		<td>1024</td>
		<td>2048</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>Broker Deregistrar</td>
		<td>1</td>
		<td>1</td>
		<td>1024</td>
		<td>2048</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>Smoke Tests</td>
		<td>1</td>
		<td>1</td>
		<td>1024</td>
		<td>2048</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>RabbitMQ on-demand broker</td>
		<td>1</td>
		<td>1</td>
		<td>1024</td>
		<td>8192</td>
		<td>1024</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>Register On-Demand Service Broker</td>
		<td>1</td>
		<td>1</td>
		<td>1024</td>
		<td>2048</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>Deregister On-Demand Service Broker</td>
		<td>1</td>
		<td>1</td>
		<td>1024</td>
		<td>2048</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>Delete All Service Instances</td>
		<td>1</td>
		<td>1</td>
		<td>1024</td>
		<td>2048</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
	<tr>
		<td>RabbitMQ</td>
		<td>Upgrade All Service Instances</td>
		<td>1</td>
		<td>1</td>
		<td>1024</td>
		<td>2048</td>
		<td>0</td>
		<td>0</td>
		<td>1</td>
	</tr>
</table>

#### Notes:
* The number of `RabbitMQ Node` can be increased if required.
* Changing the number of RabbitMQ nodes when the erlang cookie is not defined
  will restart the cluster. Check [here](install-config-pp.html#erlang) for
  more information.
